---
title: "STA 141A: Final Project"
author: "Zen Yoshida 921437539"
output: html_document
---
  
![Mouse Riding a Scooter](http://3.bp.blogspot.com/-jmYxaeosVEI/TmSHjKjlIxI/AAAAAAAAAzQ/MVIaagLrNbk/s1600/funny+mouse+pics.jpg)

***
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, fig.align = "center")
options(repos = list(CRAN="http://cran.rstudio.com/"))
library(tidyverse)
library(knitr)
library(ggplot2)

# Load the data
session <- list()
for (i in 1:18) {
  session[[i]] <- readRDS(paste("session", i, '.rds', sep=''))
}

n.session <- length(session)
```
***


# Abstract


  This project aims to analyze a subset of data collected by Steinmetz et al. (2019) from mouse experiments involving visual stimuli and neural activity recordings. The primary objective is to build a predictive model that can accurately predict the outcome of each trial based on the neural activity data and stimulus information. The project is divided into three main parts: exploratory data analysis, data integration, and model training and prediction. Through these analyses, we seek to gain insights into the data structures, neural activity patterns, and session-specific variations. The performance of the predictive model will be evaluated using test sets randomly selected from two sessions. This project report presents the findings and discusses the implications of the analysis.



# Section 1: Introduction

  The study involves 10 mice and 39 sessions. The experiments consisted of presenting visual stimuli to the mice on two screens while recording the activity of neurons in their visual cortex. Each session included multiple trials, with the mice making decisions based on the visual stimuli and receiving feedback in the form of rewards or penalties.

  The visual stimuli varied in contrast levels, ranging from 0 to 1, with 0 indicating the absence of a stimulus. The mice used a wheel controlled by their forepaws to make decisions. The outcome of their decisions depended on the relative contrast levels of the stimuli, and different rules were applied based on the specific conditions.

  The data provided in this project focuses on the spike trains of neurons recorded during the trials. Spike trains are collections of timestamps indicating the firing of neurons. Specifically, we analyze the spike trains from the onset of the stimuli to 0.4 seconds after the onset.

  For this analysis, we only consider 18 sessions. There are RDS files available, each corresponding to a specific session. These files contain records of the experiments, including the mouse's name and the date of the experiment, number of brain areas tested, the number of neurons looked at, and more. There are five variables to pay attention to in the data set, though all will not be used in this report:

**feedback_type:** type of the feedback, 1 for success and -1 for failure

**contrast_left:** contrast of the left stimulus

**contrast_right:** contrast of the right stimulus

**time:** centers of the time bins for spks

**spks:** numbers of spikes of neurons in the visual cortex in time bins defined in time

**brain_area:** area of the brain where each neuron lives
 

# Section 2: Exploratory Data Analysis

## Homogeneity and Heterogeneity
```{r, echo=FALSE, eval = TRUE}

# Create meta tibble with customizable column names (without "Date" column)
col_names <- c("Mouse Name", "# Brain Areas", "# Neurons", "# Trials", "Success Rate")
meta <- tibble(
  mouse_name = rep('name', n.session),
  n_brain_area = rep(0, n.session),
  n_neurons = rep(0, n.session),
  n_trials = rep(0, n.session),
  success_rate = rep(0, n.session)
)

# Loop through sessions and assign values to meta tibble
for (i in 1:n.session) {
  tmp <- session[[i]]
  meta[i, "mouse_name"] <- tmp$mouse_name
  meta[i, "n_brain_area"] <- length(unique(tmp$brain_area))
  meta[i, "n_neurons"] <- dim(tmp$spks[[1]])[1]
  meta[i, "n_trials"] <- length(tmp$feedback_type)
  meta[i, "success_rate"] <- mean(tmp$feedback_type + 1) / 2
}

# Change column names
colnames(meta) <- col_names

# Print meta tibble with formatted table
knitr::kable(meta, format = "html", table.attr = "class='table table-striped'", digits = 2)

```
  
  Here is a table exploring how the data is organized in the 18 Sessions. The date column is removed as it is not relevant to the project. What will be important, however, is to note how the number of brain areas and the number of neurons constantly fluctuates between Sessions. This suggests that the dataset exhibits heterogeneity between the mice. Even among Sessions of the same mice, no two Sessions are similar. 
  
***

## Neural Activities within Trials

### Average Spike Count

  To understand the structure within just one Session and one Trial, below is a simple bar graph analyzing the Average Spike Count within each brain area for Session 2, Trial 1. Recall from the graph analyzing the dataset that Session 2 has 5 Brain Areas. The 1070 neurons in this Session are located in CA1, VISl, root, VISpm, POST of the mouse brain. 
  
```{r, echo = FALSE}

# Sample definition of area colors
area.col <- c("#FF7F00", "#00BFC4", "#E69F00", "#CC79A7", "#56B4E9")

# AVERAGE EXAMPLE

i.s <- 2 # indicator for this session
i.t <- 1 # indicator for this trial 

# Calculate the spike count for each neuron during the specified trial
spk.trial <- session[[i.s]]$spks[[i.t]]
area <- session[[i.s]]$brain_area
spk.count <- apply(spk.trial, 1, sum)

# Calculate the average spike count by area
average_spike_count <- aggregate(spk.count ~ area, data = data.frame(area, spk.count), mean)
average_spike_count <- average_spike_count[order(average_spike_count$spk.count), ]

# Calculate the maximum spike count for setting ylim
max_spike_count <- max(average_spike_count$spk.count)

# Create the barplot using ggplot2 with customizations
ggplot(average_spike_count, aes(x = area, y = spk.count, fill = area)) +
  geom_bar(stat = "identity", width = 0.5) +
  scale_fill_manual(values = area.col) +
  labs(title = paste("Average Spike Count per Area in Session", i.s),
       x = "Area", y = "Average Spike Count") +
  ylim(0, max_spike_count * 1.1) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  geom_text(aes(label = format(round(spk.count, digits = 5), nsmall = 3)),
            position = position_dodge(width = 0.5), vjust = -0.5, size = 4, color = "black")




```

Average Spike Count represents the mean number of spikes in the neurons recorded for each of the brain areas. 

### Average Spike Rate

  The graph below analyzes a similar but slightly different component, the Average Spike Rate. To calculate the Average Spike Rate, you would divide the total number of spikes recorded from a neuron during a trial by the duration of the trial. This provides an estimate of the average firing rate of the neuron throughout the trial. Like the graph above, we are looking at single trial from Session 2, Trial 1.
  
```{r, echo = FALSE}


# Set the session and trial indicators
i.s <- 2  # Indicator for the session
i.t <- 1  # Indicator for the trial

# Get the spike data and area information for the specified trial
spk.trial <- session[[i.s]]$spks[[i.t]]
area <- session[[i.s]]$brain_area

# Calculate the spike rate for each neuron during this trial
spike.rate <- apply(spk.trial, 1, function(row) sum(row) / length(row))

# Create a data frame with area and spike rate information
data <- data.frame(area = area, spike_rate = spike.rate)

# Calculate the average spike rate by area
average_spike_rate <- aggregate(spike_rate ~ area, data, mean)

# Sort the areas based on average spike rate
average_spike_rate <- average_spike_rate[order(average_spike_rate$spike_rate), ]

# Calculate the maximum spike rate for setting ylim
max_spike_rate <- max(average_spike_rate$spike_rate)

# Define custom colors (5 least used colors in R)
area.col2 <- c("#FF7F00", "#00BFC4", "#E69F00", "#CC79A7", "#56B4E9")

# Create the barplot using ggplot2 with customizations and custom colors
ggplot(average_spike_rate, aes(x = area, y = spike_rate, fill = area)) +
  geom_bar(stat = "identity", width = 0.5) +
  scale_fill_manual(values = area.col2) +
  labs(title = paste("Average Spike Rate per Area in Session", i.s),
       x = "Area", y = "Spike Rate") +
  ylim(0, max_spike_rate * 1.1) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  geom_text(aes(label = format(round(spike_rate, digits = 5), nsmall = 3)),
            position = position_dodge(width = 0.5), vjust = -0.5, size = 4, color = "black")




```

  By dividing the sum of spikes in each row by the length of the row (which represents the number of time bins), the Average Spike Rate can be calculated per time bin. 
  
  Below is the number given when taken the Average Spike Rate for Session 2 Trial 1 without taking in consideration the brain areas. Because heterogeneity is due to the differences in neurons measured in each session, the information about specific neurons can be smoothed over by averaging over their activities. This will become important in Section 2, as with this we can just ignore the differences in brain areas and focus on one unifying number. 
  
```{r, echo = FALSE}

# Set the session and trial indicators
i.s <- 2  # Indicator for the session
i.t <- 1  # Indicator for the trial

# Get the spike data and area information for the specified trial
spk.trial2 <- session[[i.s]]$spks[[i.t]]
area2 <- session[[i.s]]$brain_area

# Calculate the spike rate for each neuron during this trial
all_brain_spike.rate <- apply(spk.trial2, 1, function(row) sum(row) / length(row))

# Calculate the average spike rate across all brain areas
ASR_all_brain <- mean(all_brain_spike.rate)

ASR_all_brain

```

  

### Contrast Difference

  An important consideration that will ultimately be critical when making a predictive model for Feedack Type, is contrast difference. Understanding the contrast difference can provide insights into how the mice respond to different levels of visual stimuli and how it influences their decision-making process. By examining the contrast difference, we can explore whether the mice exhibit any biases towards certain stimulus conditions or if they show preferences for specific contrasts. Contrast Difference is provided by the the difference between two variables, Contrast Left and Contrast Right. 
  
```{r, echo = FALSE}

# Set the session and trial indicators
i.s <- 2  # Indicator for the session
i.t <- 1  # Indicator for the trial
# Get the contrast data for the specified trial
contrast_left <- session[[i.s]]$contrast_left[[i.t]]
contrast_right <- session[[i.s]]$contrast_right[[i.t]]

# Calculate the contrast difference
contrast_diff <- abs(contrast_left - contrast_right)

contrast_diff

```

  In Section 2, Trial 1, the Contrast Difference is 0. There is not much to gain from the single number of a trial, so let's explore what Contrast Difference looks like between whole Sections. This can also teach the difference between trials by displaying their contribution to their Session. 
  

```{r, echo=FALSE}
# Extract Contrast Difference for Session 2, 3, and 4
session_2 <- session[[2]]
session_3 <- session[[3]]
session_4 <- session[[4]]

contrast_diff_session_2 <- session_2$contrast_left - session_2$contrast_right
contrast_diff_session_3 <- session_3$contrast_left - session_3$contrast_right
contrast_diff_session_4 <- session_4$contrast_left - session_4$contrast_right

# Combine the Contrast Difference values and create a session indicator
all_contrast_diff <- c(contrast_diff_session_2, contrast_diff_session_3, contrast_diff_session_4)
all_sessions <- factor(rep(c("Session 2", "Session 3", "Session 4"),
                          times = c(length(contrast_diff_session_2),
                                    length(contrast_diff_session_3),
                                    length(contrast_diff_session_4))))

# Combine the data into a data frame
data <- data.frame(Contrast_Difference = all_contrast_diff, Session = all_sessions)

# Create the boxplot using ggplot2
ggplot(data, aes(x = Session, y = Contrast_Difference, fill = Session)) +
  geom_boxplot() +
  scale_fill_manual(values = c("salmon3", "orchid1", "darkorchid4")) +
  labs(title = "Contrast Difference", y = "Contrast Difference") +
  theme_minimal()


```
  
  The boxplot for Session 3 is particularly strange. The median line being at the edge of the interquartile range suggests that the distribution is skewed towards lower Contrast Differences, but there are some extreme positive values that contribute to the upper whisker. There is a dot representing an outlier, meaning there was a trial that exhibted very strange behavior. 
  

# Section 3: Data Integration
## Patterns Across Sessions

### Session 2 Average Spike Rate 

  As established in Section 1, we can do things like consider the Average Spike Rate across the entire Session, not just one Trial. 
  
```{r, echo = FALSE}
# Set the session indicator
i.s <- 2  # Indicator for Session 2

# Get the spike data and area information for all trials in Session 2
spk.data <- session[[i.s]]$spks
area <- session[[i.s]]$brain_area

# Calculate the spike rate for each neuron across all trials
all_trials_spike.rate <- sapply(spk.data, function(trial) {
  spike.rate <- apply(trial, 1, function(row) sum(row) / length(row))
  mean(spike.rate)
})

# Calculate the average spike rate across all trials and brain areas in Session 2
ASR_all_trials <- mean(all_trials_spike.rate)

ASR_all_trials

```

  Taking it a step further, what is the comprehensive average spike rate for every Session? 

```{r, echo = FALSE}
# Get the total number of sessions
num_sessions <- 18

# Create an empty vector to store the average spike rates
average_spike_rates <- vector("numeric", length = num_sessions)

# Iterate over each session
for (i.s in 1:num_sessions) {
  # Get the spike data and area information for all trials in the current session
  spk.data <- session[[i.s]]$spks
  area <- session[[i.s]]$brain_area
  
  # Calculate the spike rate for each neuron across all trials in the current session
  all_trials_spike.rate <- sapply(spk.data, function(trial) {
    spike.rate <- apply(trial, 1, function(row) sum(row) / length(row))
    mean(spike.rate)
  })
  
  # Calculate the average spike rate across all trials and brain areas in the current session
  average_spike_rates[i.s] <- mean(all_trials_spike.rate)
}

# Calculate the overall average spike rate across all sessions
ASR_all_sessions <- mean(average_spike_rates)

ASR_all_sessions

```

### Comprehensive Average Spike Rate

  Due to the vast number of brain areas across the 18 Sessions, it is difficult to find easy patterns across Sessions. Nonetheless, it is possible to provide some fascinating visual information on what brain areas have higher spike rates.
  
```{r, echo = FALSE}


# Create empty lists to store the results
all_average_spike_rates <- list()

# Iterate over each session
for (i.s in 1:length(session)) {
  # Initialize variables for each session
  spk_rates <- numeric()
  areas <- character()

  # Iterate over trials within the session
  for (i.t in 1:length(session[[i.s]]$spks)) {
    # Get the spike data and area information for the specified trial
    spk.trial <- session[[i.s]]$spks[[i.t]]
    area <- session[[i.s]]$brain_area

    # Calculate the spike rate for each neuron during this trial
    spike.rate <- apply(spk.trial, 1, function(row) sum(row) / length(row))

    # Append the spike rates and areas to the session-level vectors
    spk_rates <- c(spk_rates, spike.rate)
    areas <- c(areas, area)
  }

  # Create a data frame with area and spike rate information for the session
  data <- data.frame(session = i.s, area = areas, spike_rate = spk_rates)

  # Calculate the average spike rate by area for the session
  average_spike_rate <- aggregate(spike_rate ~ session + area, data, mean)

  # Store the results for the session in the respective lists
  all_average_spike_rates[[i.s]] <- average_spike_rate
}

# Combine the average spike rate data from all sessions into a single data frame
combined_average_spike_rates <- do.call(rbind, all_average_spike_rates)

# Create a scatterplot with individual colors for each brain area
ggplot(data = combined_average_spike_rates, aes(x = factor(session), y = spike_rate, color = area)) +
  geom_point() +
  labs(title = "Comprehensive Spike Rate Analysis (All Areas)",
       x = "Session", y = "Average Spike Rate", color = "Area") +
  scale_x_discrete(breaks = unique(combined_average_spike_rates$session)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14))


```

  Plots like these allows for a comparison of the average spike rates across different brain areas within all the Sessions. It helps identify any patterns or trends in the spike rate variations between brain areas and across sessions. Furthermore, due to the large amount of brain areas, it may also be worth looking into the five most commonly seen brain areas across Sessions. After all, it is likely that the numbers from these brain areas will have the largest affect when forming a prediction in Section 3.

```{r, echo = FALSE}


# Create empty lists to store the results
all_average_spike_rates <- list()
all_max_spike_rates <- numeric()

# Iterate over each session
for (i.s in 1:length(session)) {
  # Initialize variables for each session
  spk_rates <- numeric()
  areas <- character()
  
  # Iterate over trials within the session
  for (i.t in 1:length(session[[i.s]]$spks)) {
    # Get the spike data and area information for the specified trial
    spk.trial <- session[[i.s]]$spks[[i.t]]
    area <- session[[i.s]]$brain_area
    
    # Calculate the spike rate for each neuron during this trial
    spike.rate <- apply(spk.trial, 1, function(row) sum(row) / length(row))
    
    # Append the spike rates and areas to the session-level vectors
    spk_rates <- c(spk_rates, spike.rate)
    areas <- c(areas, area)
  }
  
  # Create a data frame with area and spike rate information for the session
  data <- data.frame(area = areas, spike_rate = spk_rates)
  
  # Calculate the average spike rate by area for the session
  average_spike_rate <- aggregate(spike_rate ~ area, data, mean)
  
  # Store the results for the session in the respective lists
  all_average_spike_rates[[i.s]] <- average_spike_rate
}

# Combine the average spike rate data from all sessions into a single data frame
combined_average_spike_rates <- do.call(rbind, all_average_spike_rates)


 
# Get the top 5 most common brain areas across all sessions
top_areas <- head(sort(table(combined_average_spike_rates$area), decreasing = TRUE), 5)

# Filter the combined data for the top areas
top_data <- combined_average_spike_rates[combined_average_spike_rates$area %in% names(top_areas), ]

# Calculate the average spike rate by area across all sessions for the top areas
average_spike_rate <- aggregate(spike_rate ~ area, top_data, mean)

# Sort the areas based on average spike rate
average_spike_rate <- average_spike_rate[order(average_spike_rate$spike_rate), ]

# Calculate the maximum spike rate for setting ylim
max_spike_rate <- max(average_spike_rate$spike_rate)

# Create the barplot using ggplot2 with customizations
ggplot(average_spike_rate, aes(x = area, y = spike_rate, fill = area)) +
  geom_bar(stat = "identity", width = 0.5) +
  scale_fill_manual(values = area.col) +
  labs(title = "Comprehensive Spike Rate Analysis (Top 5 Areas)",
       x = "Area", y = "Average Spike Rate") +
  ylim(0, max_spike_rate * 1.1) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  geom_text(aes(label = format(round(spike_rate, digits = 5), nsmall = 3)),
            position = position_dodge(width = 0.5), vjust = -0.5, size = 4, color = "black")


```

  Overall, due to the incompatibility of brain areas across every Session, it would be best to utilize the technique of finding the average across the brain areas. This is what will be used in the prediction modeling. 


***

# Section 4: Prediction Modeling

## Logistic Regression

 To begin fitting a Logistic Regression Model factoring the Average Spike Rate and Contrast Difference as variables and the Feedback Type as the response, a new dataset called combined_dataset is created. It is essential that the glm() function set to the binomial distribution only takes in the binary values 0 or 1 as the response, so the Feedback Type variable must be changed from -1 to 1 to 0 to 1. 
 
```{r, echo = FALSE}

# Initialize an empty dataset to store contrast differences
contrast_diff_dataset <- data.frame(contrast_diff = numeric(0))

# Loop through each session
for (i.s in 1:18) {
  # Loop through each trial in the current session
  for (i.t in 1:length(session[[i.s]]$contrast_left)) {
    # Get the contrast data for the current trial
    contrast_left <- session[[i.s]]$contrast_left[[i.t]]
    contrast_right <- session[[i.s]]$contrast_right[[i.t]]
    
    # Calculate the contrast difference
    contrast_diff <- abs(contrast_left - contrast_right)
    
    # Append the contrast difference to the dataset
    contrast_diff_dataset <- rbind(contrast_diff_dataset, data.frame(contrast_diff))
  }
}

 
```

```{r, echo = FALSE}

# Initialize an empty dataset to store average spike rates
avg_spike_rate_dataset <- data.frame(avg_spike_rate = numeric(0))

# Loop through each session
for (i.s in 1:18) {
  # Get the number of trials in the current session
  num_trials <- length(session[[i.s]]$spks)
  
  # Loop through each trial in the session
  for (i.t in 1:num_trials) {
    # Get the spike data for the current trial
    spk.trial <- session[[i.s]]$spks[[i.t]]
    
    # Calculate the spike rate for each neuron during this trial
    neuron_spike_rate <- apply(spk.trial, 1, function(row) sum(row) / length(row))
    
    # Calculate the average spike rate across all neurons
    avg_spike_rate <- mean(neuron_spike_rate)
    
    # Append the average spike rate to the dataset
    avg_spike_rate_dataset <- rbind(avg_spike_rate_dataset, data.frame(avg_spike_rate))
  }
}

 
```

```{r, echo = FALSE}
# Initialize an empty dataset to store feedback types
feedback_type_dataset <- data.frame(feedback_type = character(0))

# Loop through each session
for (i.s in 1:18) {
  # Get the number of trials in the current session
  num_trials <- length(session[[i.s]]$spks)
  
  # Loop through each trial in the session
  for (i.t in 1:num_trials) {
    # Get the feedback type for the current trial
    feedback_type <- session[[i.s]]$feedback_type[[i.t]]
    
    # Check if the feedback type is valid (-1 or 1)
    if (feedback_type %in% c(-1, 1)) {
      # Rename -1 to 0 and 1 to 1
      feedback_type <- ifelse(feedback_type == -1, 0, 1)
      
      # Append the feedback type to the dataset
      feedback_type_dataset <- rbind(feedback_type_dataset, data.frame(feedback_type))
    }
  }
}

# Rename feedback_type column to "outcome"
names(feedback_type_dataset) <- "outcome"

# Combine the average spike rate dataset, contrast difference dataset, and feedback type dataset
combined_dataset <- cbind(avg_spike_rate_dataset, contrast_diff_dataset, feedback_type_dataset)

# View the combined dataset
head(combined_dataset)

```

  Next, the Logistic Regression Model is tested prior to the release of the datasets on Monday, to confirm things are working fluidly. To do this, we take the first 100 trials in the combined_dataset and set that aside in a test_dataset to use. The rest of the dataset is set aside in the train_dataset and used in the glm() function.

```{r, echo = FALSE}
set.seed(123)

# Define the number of trials to set aside for testing
num_test_trials <- 100

# Split the data into training and test datasets
test_dataset <- combined_dataset[1:num_test_trials, ]
train_dataset <- combined_dataset[(num_test_trials + 1):nrow(combined_dataset), ]

# Perform logistic regression on the training dataset
model <- glm(outcome ~ avg_spike_rate + contrast_diff, data = train_dataset, family = binomial)

estimates0 <- coef(model)
standarderrors0 <- sqrt(diag(vcov(model)))

cat("Estimates:\n")
cat(estimates0, "\n")

cat("Standard Errors:\n")
cat(standarderrors0, "\n")

# Predict probabilities using the model on the test dataset
predicted_probs <- predict(model, test_dataset, type = "response")

predicted_labels <- ifelse(predicted_probs >= 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix <- table(test_dataset$outcome, predicted_labels)
confusion_matrix

# Calculate misclassification rate
misclassification_rate <- mean(test_dataset$outcome != predicted_labels)
misclassification_rate

# Calculate the F1 score
TP <- sum(test_dataset$outcome == 1 & predicted_labels == 1)  # True Positives
FP <- sum(test_dataset$outcome == 0 & predicted_labels == 1)  # False Positives
FN <- sum(test_dataset$outcome == 1 & predicted_labels == 0)  # False Negatives

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score

```
  
  With a 34% misclassification rate, the result of this initial testing was only mediocre. Under two-thirds of the test were classified correctly. A 0.79 F1 Score represents an acceptable but not great trade-off between correctly identifying positive instances (precision) and capturing all positive instances (recall). This suggests that the model may have a bias or may need further refinement to improve its performance. Somewhat disappointing, but nonetheless interesting. 


# Section 5: Prediction Modeling with Test

  For the culmination of the project, two test datasets are released. This means we can use the entirety of combined_dataset for our training. Combined_dataset_test is created to house the test datasets in the same format as combined_dataset.

```{r, echo = FALSE}
# Load the data
test <- list()
for (i in 1:2) {
  test[[i]] <- readRDS(paste("test", i, '.rds', sep=''))
}

# Initialize an empty dataset to store contrast differences
contrast_diff_dataset_test <- data.frame(contrast_diff = numeric(0))

# Loop through each session
for (i.s in 1:2) {
  # Loop through each trial in the current session
  for (i.t in 1:length(test[[i.s]]$contrast_left)) {
    # Get the contrast data for the current trial
    contrast_left <- test[[i.s]]$contrast_left[[i.t]]
    contrast_right <- test[[i.s]]$contrast_right[[i.t]]
    
    # Calculate the contrast difference
    contrast_diff <- abs(contrast_left - contrast_right)
    
    # Append the contrast difference to the dataset
    contrast_diff_dataset_test <- rbind(contrast_diff_dataset_test, data.frame(contrast_diff))
  }
}

```

```{r, echo = FALSE}
# Initialize an empty dataset to store average spike rates
avg_spike_rate_dataset_test <- data.frame(avg_spike_rate = numeric(0))

# Loop through each session
for (i.s in 1:2) {
  # Get the number of trials in the current session
  num_trials <- length(test[[i.s]]$spks)
  
  # Loop through each trial in the session
  for (i.t in 1:num_trials) {
    # Get the spike data for the current trial
    spk.trial <- test[[i.s]]$spks[[i.t]]
    
    # Calculate the spike rate for each neuron during this trial
    neuron_spike_rate <- apply(spk.trial, 1, function(row) sum(row) / length(row))
    
    # Calculate the average spike rate across all neurons
    avg_spike_rate <- mean(neuron_spike_rate)
    
    # Append the average spike rate to the dataset
    avg_spike_rate_dataset_test <- rbind(avg_spike_rate_dataset_test, data.frame(avg_spike_rate))
  }
}


```

```{r, echo = FALSE}
# Initialize an empty dataset to store feedback types
feedback_type_dataset_test <- data.frame(feedback_type = character(0))

# Loop through each session
for (i.s in 1:2) {
  # Get the number of trials in the current session
  num_trials <- length(test[[i.s]]$spks)
  
  # Loop through each trial in the session
  for (i.t in 1:num_trials) {
    # Get the feedback type for the current trial
    feedback_type <- test[[i.s]]$feedback_type[[i.t]]
    
    # Check if the feedback type is valid (-1 or 1)
    if (feedback_type %in% c(-1, 1)) {
      # Rename -1 to 0 and 1 to 1
      feedback_type <- ifelse(feedback_type == -1, 0, 1)
      
      # Append the feedback type to the dataset
      feedback_type_dataset_test <- rbind(feedback_type_dataset_test, data.frame(feedback_type))
    }
  }
}


# Rename feedback_type column to "outcome"
names(feedback_type_dataset_test) <- "outcome"

# Combine the average spike rate dataset, contrast difference dataset, and feedback type dataset
combined_dataset_test <- cbind(avg_spike_rate_dataset_test, contrast_diff_dataset_test, feedback_type_dataset_test)

# View the combined dataset
head(combined_dataset_test)

```


```{r, echo = FALSE}


# Perform logistic regression on the training dataset
model2 <- glm(outcome ~ avg_spike_rate + contrast_diff, data = combined_dataset, family = binomial)

estimates <- coef(model2)
standarderrors <- sqrt(diag(vcov(model2)))



cat("Estimates:\n")
cat(estimates, "\n")

cat("Standard Errors:\n")
cat(standarderrors, "\n")


# Predict probabilities using the model on the test dataset
predicted_probs <- predict(model2, combined_dataset_test, type = "response")


predicted_labels <- ifelse(predicted_probs >= 0.5, 1,0)

# Create a confusion matrix
confusion_matrix <- table(combined_dataset_test$outcome, predicted_labels)
confusion_matrix

# Calculate misclassification rate
misclassification_rate <-  mean(combined_dataset_test$outcome != predicted_labels)
misclassification_rate

# Calculate the F1 score
TP <- sum(combined_dataset_test$outcome == 1 & predicted_labels == 1)  # True Positives
FP <- sum(combined_dataset_test$outcome == 0 & predicted_labels == 1)  # False Positives
FN <- sum(combined_dataset_test$outcome == 1 & predicted_labels == 0)  # False Negatives

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score

```

  With a misclassification rate of 0.275, the final model improved slightly compared to the practice run. An F1 score was also included, which at 0.84 indicates a relatively good balance between precision and recall. This suggests that the model has a reasonable trade-off between correctly identifying positive instances (precision) and capturing all positive instances (recall).


```{r, echo = FALSE}
library(ggplot2)

# Create a dataframe for the predicted probabilities and true outcomes
results <- data.frame(predicted_probs, true_outcome = combined_dataset_test$outcome)

# Create a scatter plot of predicted probabilities against true outcomes
ggplot(results, aes(x = predicted_probs, y = true_outcome)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(x = "Predicted Probabilities", y = "True Outcome") +
  ggtitle("Logistic Regression Results") +
  theme_minimal()

```

  Lastly, a graph is included to analyze the effectiveness of the Logistic Regression Model. The scatter plot helps visualize the relationship between the predicted probabilities and the true outcomes. It provides a visual representation of how well the logistic regression model predicts the outcomes. The points are scattered across the plot, indicating the varying predicted probabilities for different instances.

  Additionally, a smooth curve is fitted to the scatter plot using the logistic regression method. This curve represents the estimated relationship between the predicted probabilities and the true outcomes, based on the model. The curve provides insights into the overall trend and pattern in the data.


# Discussion
  
  Overall, utilizing the Average Spike Rate and Contrast Difference as the variables may not have been the best option to assess Feedback Type. 27.5% misclassification rate is not horrible, but it also suggests a high amount of possible improvement. The 0.84 F1 Score further tells me that I was headed in a good direction. The estimates and standard errors were also not wildly suspicious. Despite not being able to create the perfect prediction model, the process taught me significantly, and I am proud of what I was able to do. Given more experience and time, I would have liked to explore other variables and see how they would improve the model. More exploration into utilizing brain areas may have been warranted as well.
  
  
![My Computer Mouse After Finishing This Project](http://1.bp.blogspot.com/-d_dH5GJhtuk/TmSHlnpFZEI/AAAAAAAAAzc/CYVUSfCqxSw/s1600/Funny+pictures+of+computer+mouse.jpg)  

# Reference

  Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266â€“273 (2019). https://doi.org/10.1038/s41586-019-1787-x


